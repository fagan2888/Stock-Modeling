{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {
    "collapsed": true
   },
   "level": 2,
   "source": [
    "Final Project: Stock Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook models stock data that has been pre-processed with news sentiment data, then predicts the Closing, High, and Low values for the current buisness day and following business day. Below is the process for preparing the data then modeling the data for the best predictions."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Section 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "# Set Console formatting for panda prints\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# **********************************************************************************************************************\n",
    "# Modeling / Prepare Data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/mwilchek/Stock-Modeling/master/DJ_NEWS_SENTIMENT_DATA.csv')\n",
    "data['Cycle_Change'] = data.Max_Sentiment.eq(data.Max_Sentiment.shift())\n",
    "dummies = pd.get_dummies(data.Cycle_Change)\n",
    "data.join(dummies)\n",
    "data_tomorrow = data\n",
    "\n",
    "# Move certain columns up by one row for data_tomorrow\n",
    "data_tomorrow.Anger = data_tomorrow.Anger.shift(+1)\n",
    "data_tomorrow.Anticipation = data_tomorrow.Anticipation.shift(+1)\n",
    "data_tomorrow.Disgust = data_tomorrow.Disgust.shift(+1)\n",
    "data_tomorrow.Fear = data_tomorrow.Fear.shift(+1)\n",
    "data_tomorrow.Joy = data_tomorrow.Joy.shift(+1)\n",
    "data_tomorrow.Sadness = data_tomorrow.Sadness.shift(+1)\n",
    "data_tomorrow.Surprise = data_tomorrow.Surprise.shift(+1)\n",
    "data_tomorrow.Trust = data_tomorrow.Trust.shift(+1)\n",
    "data_tomorrow.Negative = data_tomorrow.Negative.shift(+1)\n",
    "data_tomorrow.Positive = data_tomorrow.Positive.shift(+1)\n",
    "data_tomorrow.Max_Sentiment = data_tomorrow.Max_Sentiment.shift(+1)\n",
    "data_tomorrow.Sentiment_Proportion = data_tomorrow.Sentiment_Proportion.shift(+1)\n",
    "\n",
    "# Delete the first row of data_tomorrow\n",
    "data_tomorrow.drop(data_tomorrow.head(1).index, inplace=True)\n",
    "\n",
    "train_data = data[:-1]  # train data\n",
    "today_record = data.tail(1)  # test data (validate current day and predict from following day)\n",
    "train_data_tomorrow = data_tomorrow[:-1]  # train data\n",
    "tomorrow_record = data_tomorrow.tail(1)  # test data (validate current day and predict from following day)\n",
    "\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Local Functions for Accuracy Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Local method to get Margin of Error\n",
    "def get_change(current, previous):\n",
    "    if current == previous:\n",
    "        return 100.0\n",
    "    try:\n",
    "        return (abs(current - previous) / previous) * 100.0\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Section 2: Data Modeling Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we created a pipeline of Regressor type models with a number of parameters that we tune to hopefully find an accurate model for predicting the closing value. We hope that with good results, we can mimic the process for predicting the High and Low values for the current and next business day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# MODELING EXPLORATION #################################################################################################\n",
    "# Testing best model for f(x) = Close ~ Features\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Get Feature values\n",
    "x = data[['Open', 'High', 'Low', 'False', 'True']].values\n",
    "\n",
    "# Get Target values\n",
    "y = data['Close'].values\n",
    "\n",
    "regression_models = {'lr': LinearRegression(n_jobs=-1),\n",
    "                     'mlp': MLPRegressor(random_state=0),\n",
    "                     'dt': DecisionTreeRegressor(random_state=0),\n",
    "                     'rf': RandomForestRegressor(random_state=0, n_jobs=-1),\n",
    "                     'svr': SVR(max_iter=-1)}\n",
    "\n",
    "pipe_regrs = {}\n",
    "\n",
    "# Create list of pipeline models to test with that standardize the data\n",
    "for name, regression_models in regression_models.items():\n",
    "    pipe_regrs[name] = Pipeline([('StandardScaler', StandardScaler()), ('regr', regression_models)])\n",
    "\n",
    "param_grids = {}\n",
    "\n",
    "# Linear Regression Parameter Options:\n",
    "param_grid = [{'regr__normalize': ['True']},\n",
    "              {'regr__normalize': ['False']}]\n",
    "\n",
    "# Add Linear Regression Parameters to dictionary grid\n",
    "param_grids['lr'] = param_grid\n",
    "\n",
    "# MLP Parameter Options:\n",
    "alpha_range = [10 ** i for i in range(-4, 5)]\n",
    "\n",
    "param_grid = [{'regr__hidden_layer_sizes': [10, 100, 200]}]\n",
    "\n",
    "# Add Multi-layer Perceptron Parameters to dictionary grid\n",
    "param_grids['mlp'] = param_grid\n",
    "\n",
    "# Decision Tree Regression Parameter Options:\n",
    "param_grid = [{'regr__criterion': ['mse', 'mae'],\n",
    "               'regr__min_samples_split': [2, 6, 10],\n",
    "               'regr__min_samples_leaf': [1, 6, 10],\n",
    "               'regr__max_features': ['auto', 'sqrt', 'log2']}]\n",
    "\n",
    "# Add Decision Tree Parameters to dictionary grid\n",
    "param_grids['dt'] = param_grid\n",
    "\n",
    "# Random Forest Regression Parameter Options:\n",
    "param_grid = [{'regr__n_estimators': [10, 100],\n",
    "               'regr__criterion': ['mse', 'mae'],\n",
    "               'regr__min_samples_split': [2, 6, 10],\n",
    "               'regr__min_samples_leaf': [1, 6, 10],\n",
    "               'regr__max_features': ['auto', 'sqrt', 'log2']}]\n",
    "\n",
    "# Add Random Forest Parameters to dictionary grid\n",
    "param_grids['rf'] = param_grid\n",
    "\n",
    "# Support Vector Machine (SVM) Parameter Options:\n",
    "param_grid = [{'regr__C': [0.1, 1, 10],\n",
    "               'regr__gamma': [0.1, 1, 10],\n",
    "               'regr__kernel': ['linear', 'poly', 'rbf', 'sigmoid']}]\n",
    "\n",
    "# Add SVM Parameters to dictionary grid\n",
    "param_grids['svr'] = param_grid\n",
    "\n",
    "# The list of [best_score_, best_params_, best_estimator_]\n",
    "best_score_param_estimators = []\n",
    "\n",
    "# Scoring Param: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "# For each regression\n",
    "for name in pipe_regrs.keys():\n",
    "    # GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe_regrs[name],\n",
    "                      param_grid=param_grids[name],\n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      n_jobs=1,\n",
    "                      cv=None)\n",
    "    print(\"Modeling: \" + str(pipe_regrs[name]))\n",
    "    # Fit the pipeline\n",
    "    gs = gs.fit(x, y)\n",
    "\n",
    "    # Update best_score_param_estimators\n",
    "    best_score_param_estimators.append([gs.best_score_, gs.best_params_, gs.best_estimator_])\n",
    "    print(\"Modeling Completed - Appending scores...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the user's computer, the amount of time to complete the Pipeline can be between 5-10mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort best_score_param_estimators in descending order of the best_score_\n",
    "best_score_param_estimators = sorted(best_score_param_estimators, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# For each [best_score_, best_params_, best_estimator_]\n",
    "for best_score_param_estimator in best_score_param_estimators:\n",
    "    # Print out [best_score_, best_params_, best_estimator_], where best_estimator_ is a pipeline\n",
    "    # Since we only print out the type of classifier of the pipeline\n",
    "    print([best_score_param_estimator[0], best_score_param_estimator[1],\n",
    "           type(best_score_param_estimator[2].named_steps['regr'])], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the '{'regr__normalize': 'True'}, <class 'sklearn.linear_model.base.LinearRegression'>]' model from the Pipeline was the best scored. Let us practice predicting the cclosing value of the stock with a Linear Regression model with the best tuned parameters from our GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare best model from GridSearchCV where normalize set to True is the default parameter\n",
    "lr = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# Fit the model with our data\n",
    "lr = lr.fit(x, y)\n",
    "\n",
    "# Predict on Today Close\n",
    "today_close = today_record[['Open', 'High', 'Low', 'False', 'True']].values\n",
    "y_pred = lr.predict(today_close)\n",
    "\n",
    "# Print Results\n",
    "print(\"Actual Closing Value: \" + str(today_record['Close'].values[0]))\n",
    "print(\"Predicted Closing Value: \" + str(y_pred[0]))\n",
    "\n",
    "error = get_change(y_pred[0], today_record['Close'].values[0])\n",
    "print(\"Accuracy error for prediction: \" + str(round(error, 4)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction results were pretty close to the actual. However, based on academic research we also want to explore OLS regression to see if there is any noticeable change in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********************************************************************************************#\n",
    "# OLS Regression Test\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define formula string for Stats-model API\n",
    "formula = 'Close ~ Open + High + Low + False + True'\n",
    "\n",
    "# Define Training Data\n",
    "dta = train_data[['Close', 'Open', 'High', 'Low', 'Anger', 'Anticipation',\n",
    "                  'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                  'Trust', 'Negative', 'Positive', 'Cycle_Change', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Set the Model\n",
    "ols_today_close_model = smf.ols(formula=formula, data=dta).fit()\n",
    "\n",
    "# Print results\n",
    "print(ols_today_close_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model is clearly overfit based on the Adj. R Squared equaling to 1. In order to prevent overfitting, we will revise our model with a regularized fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Model with Regularized Fit to prevent over-fitting; alpha and weight values were set.\n",
    "olsUpdate_today_close = smf.ols(formula=formula, data=dta).fit_regularized(alpha=10, L1_wt=.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we practice predicting on today's record with the revised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "print(today_record)\n",
    "\n",
    "olsUpdate_today_close_prediction = olsUpdate_today_close.predict(today_record)\n",
    "\n",
    "# Show Updated Model\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig = sm.graphics.plot_partregress_grid(olsUpdate_today_close_prediction, fig=fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear an OLS Regularized fit regression model is quite accurate as well to the actual value based on the above plots. If we can hyperpameter tune the sentiment values in our formula, and the alpha and weight values for the regularized parameters we may be able to create a strong model for predicting stock values with news sentiment data as a significant relationship."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Section 3: Hyperparameter Tuning for OLS Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will create custom local methods that will find the most significant sentiment for today's stock values in Close, High, and Low using a RandForestRegressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "########################################################################################################################\n",
    "# TODAY: Local method to identify most significant feature in dataset compared to y\n",
    "def identify_sig_feature_4_today(y_variable, graph_data):\n",
    "    \n",
    "    # Split Data Into X, which are ALL the features\n",
    "    x = data.iloc[:, 9:18].values\n",
    "\n",
    "    # Split Data Into y, which are the associated targets/classifications; looking at Volume\n",
    "    y = data[np.unicode(y_variable)].values\n",
    "\n",
    "    # Get the Column Names for Sentiment, Ignore Index\n",
    "    feat_labels = data.columns[9:19]\n",
    "\n",
    "    # Randomly choose 20% of the data for testing; want a large train set (set random_state as 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Declare the StandardScaler\n",
    "    std_scaler = StandardScaler()\n",
    "\n",
    "    # Standardize the features in the training data\n",
    "    X_train = std_scaler.fit_transform(X_train)\n",
    "\n",
    "    # Standardize the features in testing data\n",
    "    X_test = std_scaler.transform(X_test)\n",
    "\n",
    "    # Start The Random Forest Regressor\n",
    "    treereg = RandomForestRegressor(n_estimators=100, max_depth=11, random_state=0)\n",
    "\n",
    "    # Execute The Data With The Random Forest Regressor\n",
    "    treereg.fit(X_train, y_train)\n",
    "\n",
    "    print('The accuracy of the random forest for today sentiment is: ' + str(treereg.score(X_test, y_test)))\n",
    "\n",
    "    # Get The Important Features From The Regressor\n",
    "    importances = treereg.feature_importances_\n",
    "\n",
    "    # Sort The Features By The Most Important\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Return data\n",
    "    df_cols = ['Sentiment', 'Importance']\n",
    "    master_df = pd.DataFrame(columns=df_cols)\n",
    "\n",
    "    for f in range(x.shape[1]):\n",
    "        sentiment = feat_labels[f]\n",
    "        importance = importances[indices[f]]\n",
    "        temp_data = {'Sentiment': sentiment,\n",
    "                     'Importance': importance}\n",
    "        master_df = master_df.append(temp_data, ignore_index=True)\n",
    "\n",
    "    highest_sentiment = master_df['Sentiment'].iloc[0]\n",
    "    highest_importance = master_df['Importance'].iloc[0]\n",
    "\n",
    "    if graph_data == \"TRUE\":\n",
    "        # Output Data As A Plot for Overall Data set\n",
    "        plt.title('Today Feature Importances ' + np.unicode(y_variable))\n",
    "        plt.bar(range(x.shape[1]), importances[indices], color='lightblue', align='center')\n",
    "        plt.xticks(range(x.shape[1]), feat_labels, rotation=90)\n",
    "        plt.xlim([-1, x.shape[1]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return highest_sentiment, highest_importance\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# TOMORROW: Local method to identify most significant feature in dataset compared to y\n",
    "def identify_sig_feature_4_tomorrow(y_variable, graph_data):\n",
    "    \n",
    "    # Split Data Into X, which are ALL the features\n",
    "    x = data_tomorrow.iloc[:, 9:18].values\n",
    "\n",
    "    # Split Data Into y, which are the associated targets/classifications; looking at Volume\n",
    "    y = data_tomorrow[np.unicode(y_variable)].values\n",
    "\n",
    "    # Get the Column Names for Sentiment, Ignore Index\n",
    "    feat_labels = data_tomorrow.columns[9:19]\n",
    "\n",
    "    # Randomly choose 20% of the data for testing; want a large train set (set random_state as 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # Declare the StandardScaler\n",
    "    std_scaler = StandardScaler()\n",
    "\n",
    "    # Standardize the features in the training data\n",
    "    X_train = std_scaler.fit_transform(X_train)\n",
    "\n",
    "    # Standardize the features in testing data\n",
    "    X_test = std_scaler.transform(X_test)\n",
    "\n",
    "    # Start The Random Forest Regressor\n",
    "    treereg = RandomForestRegressor(n_estimators=100, max_depth=11, random_state=1)\n",
    "\n",
    "    # Execute The Data With The Random Forest Regressor\n",
    "    treereg.fit(X_train, y_train)\n",
    "\n",
    "    print('The accuracy of the random forest for tomorrow sentiment is: ' + str(treereg.score(X_test, y_test)))\n",
    "\n",
    "    # Get The Important Features From The Regressor\n",
    "    importances = treereg.feature_importances_\n",
    "\n",
    "    # Sort The Features By The Most Important\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Return data\n",
    "    df_cols = ['Sentiment', 'Importance']\n",
    "    master_df = pd.DataFrame(columns=df_cols)\n",
    "\n",
    "    for f in range(x.shape[1]):\n",
    "        sentiment = feat_labels[f]\n",
    "        importance = importances[indices[f]]\n",
    "        temp_data = {'Sentiment': sentiment,\n",
    "                     'Importance': importance}\n",
    "        master_df = master_df.append(temp_data, ignore_index=True)\n",
    "\n",
    "    highest_sentiment = master_df['Sentiment'].iloc[0]\n",
    "    highest_importance = master_df['Importance'].iloc[0]\n",
    "\n",
    "    if graph_data == \"TRUE\":\n",
    "        # Output Data As A Plot for Overall Data set\n",
    "        plt.title('Tomorrow Feature Importances ' + np.unicode(y_variable))\n",
    "        plt.bar(range(x.shape[1]), importances[indices], color='lightblue', align='center')\n",
    "        plt.xticks(range(x.shape[1]), feat_labels, rotation=90)\n",
    "        plt.xlim([-1, x.shape[1]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return highest_sentiment, highest_importance\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# Local method to correctly retrieve appropriate paramters for Regularized Fit Regression based on Ridge Regression\n",
    "def get_fit_regression_params(significant_sentiment, target_variable, sentiment_value):\n",
    "    \n",
    "    # Define the data needed for this section, and as defined by highest_sentiment\n",
    "    x = data[significant_sentiment].values.reshape(-1, 1)\n",
    "\n",
    "    y = data[np.unicode(target_variable)].values  # used to be just data.High\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    x_std = scaler.fit_transform(x)\n",
    "    \n",
    "    # Create ridge regression with alpha values from .1 to 10.0, in increments of 0.1\n",
    "    regr_cv = RidgeCV(alphas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0,\n",
    "                              1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0,\n",
    "                              2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0,\n",
    "                              3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.0,\n",
    "                              4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5.0,\n",
    "                              5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6.0,\n",
    "                              6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 7.0,\n",
    "                              7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9, 8.0,\n",
    "                              8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9.0,\n",
    "                              9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9, 10.0])\n",
    "\n",
    "    # Place x and y variables in the proper format for model_cv.\n",
    "    y = np.array(y)\n",
    "    x_std = x_std.reshape((len(y), 1))\n",
    "    y = y.reshape((len(y), 1))\n",
    "\n",
    "    # Determine the best alpha value to use.\n",
    "    model_cv = regr_cv.fit(x_std, y)\n",
    "    alpha_val_today = model_cv.alpha_\n",
    "\n",
    "    # Set the L1 value based on significant_sentiment_value\n",
    "    if sentiment_value >= 0.7:\n",
    "        weight_value = 0.4\n",
    "    elif sentiment_value >= 0.4:\n",
    "        weight_value = 0.5\n",
    "    else:\n",
    "        weight_value = 0.6\n",
    "\n",
    "    return alpha_val_today, weight_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the local functions to tune the parameters for our prediction formula and regularized fit parameters, let's practice again on the Today's closing value."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Section 4: OLS Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# MODELING 4 TODAY #####################################################################################################\n",
    "# Prepare formula to predict closing of stock data for today\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'Close' for today\n",
    "highest_sentiment1_today, significant_value1_today = identify_sig_feature_4_today(\"Close\", \"False\")\n",
    "\n",
    "# Update our 'Close' formula with the most significant feature based on today's data\n",
    "formula = ('Close ~ Open + High + Low + ' + np.unicode(highest_sentiment1_today))\n",
    "\n",
    "# Update our training data for 'Close'\n",
    "dta = train_data[['Close', 'Open', 'High', 'Low', 'Anger', 'Anticipation',\n",
    "                  'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                  'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'Close' and today.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment1_today, \"Close\", significant_value1_today)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm1_today = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig1 = plt.figure(figsize=(12, 8))\n",
    "fig1 = sm.graphics.plot_partregress_grid(lm1_today, fig=fig1)\n",
    "#fig1\n",
    "#fig1.savefig('Today_Close_Regression.png')\n",
    "\n",
    "# Predicts closing value based on train data and model above\n",
    "today_close_prediction = lm1_today.predict(today_record)\n",
    "\n",
    "########################################################################################################################\n",
    "# Prepare formula to predict High of stock data for today\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'High' for today \n",
    "highest_sentiment2_today, significant_value2_today = identify_sig_feature_4_today(\"High\", \"False\")\n",
    "\n",
    "# Update our 'High' formula with the most significant feature based on today's data\n",
    "formula = ('High ~ Open + Close + Low + ' + np.unicode(highest_sentiment2_today))\n",
    "\n",
    "# Update our training data for 'High'\n",
    "dta = train_data[['High', 'Open', 'Close', 'Low', 'Anger', 'Anticipation',\n",
    "                  'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                  'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'High' and today.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment2_today, \"High\", significant_value2_today)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm2_today = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig2 = plt.figure(figsize=(12, 8))\n",
    "fig2 = sm.graphics.plot_partregress_grid(lm2_today, fig=fig2)\n",
    "#fig2\n",
    "#fig2.savefig('Today_High_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts high value based on train data and model above\n",
    "today_high_prediction = lm2_today.predict(today_record)\n",
    "\n",
    "########################################################################################################################\n",
    "# Prepare formula to predict Low of stock data for today\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'Low' for today \n",
    "highest_sentiment3_today, significant_value3_today = identify_sig_feature_4_today(\"Low\", \"False\")\n",
    "\n",
    "# Update our 'Low' formula with the most significant feature based on today's data\n",
    "formula = ('Low ~ Open + Close + High + ' + np.unicode(highest_sentiment3_today))\n",
    "\n",
    "# Update our training data for 'Low'\n",
    "dta = train_data[['Low', 'Open', 'Close', 'High', 'Anger', 'Anticipation',\n",
    "                  'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                  'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'Low' and today.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment3_today, \"Low\", significant_value3_today)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm3_today = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig3 = plt.figure(figsize=(12, 8))\n",
    "fig3 = sm.graphics.plot_partregress_grid(lm3_today, fig=fig3)\n",
    "#fig3\n",
    "#fig3.savefig('Today_Low_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts Low value based on train data and model above\n",
    "today_low_prediction = lm3_today.predict(today_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our predicting, now let's print results and compare with actuals..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Close value for today's stock is predicted to be: \" + str(today_close_prediction.iloc[0]))\n",
    "print(\"The High value for today's stock is predicted to be: \" + str(today_high_prediction.iloc[0]))\n",
    "print(\"The Low value for today's stock is predicted to be: \" + str(today_low_prediction.iloc[0]))\n",
    "print(\"\")\n",
    "print(\"ACTUAL Close value for today: \" + str(today_record['Close'].iloc[0]))\n",
    "print(\"ACTUAL High value for today: \" + str(today_record['High'].iloc[0]))\n",
    "print(\"ACTUAL Low value for today: \" + str(today_record['Low'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our results are pretty good. Let's re-use this methodology to actually try to predict for tomorrow's Closing, High, and Low values of the stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# MODELING 4 NEXT DAY###################################################################################################\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'Close' for tomorrow \n",
    "highest_sentiment1_tom, significant_value1_tom = identify_sig_feature_4_tomorrow(\"Close\", \"False\")\n",
    "\n",
    "# Update our 'Close' formula with the most significant feature based on tomorrow\n",
    "formula = ('Close ~ Open + High + Low + ' + np.unicode(highest_sentiment1_tom))\n",
    "\n",
    "# Update our training data for 'Close'\n",
    "dta = train_data_tomorrow[['Close', 'Open', 'High', 'Low', 'Anger', 'Anticipation',\n",
    "                           'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                           'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'Close' and tomorrow.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment1_tom, \"Close\", significant_value1_tom)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm1_tom = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig4 = plt.figure(figsize=(12, 8))\n",
    "fig4 = sm.graphics.plot_partregress_grid(lm1_tom, fig=fig4)\n",
    "#fig4\n",
    "#fig4.savefig('Tomorrow_Close_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts closing value based on train data and model above\n",
    "close_prediction_tom = lm1_tom.predict(tomorrow_record)\n",
    "\n",
    "########################################################################################################################\n",
    "# Prepare formula to predict High of stock data for tomorrow\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'High' for tomorrow \n",
    "highest_sentiment2_tom, significant_value2_tom = identify_sig_feature_4_tomorrow(\"High\", \"False\")\n",
    "\n",
    "# Update our 'High' formula with the most significant feature based on tomorrow\n",
    "formula = ('High ~ Open + Close + Low + ' + np.unicode(highest_sentiment2_tom))\n",
    "\n",
    "# Update our training data for 'High'\n",
    "dta = train_data_tomorrow[['High', 'Open', 'Close', 'Low', 'Anger', 'Anticipation',\n",
    "                           'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                           'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'High' and tomorrow.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment2_tom, \"High\", significant_value2_tom)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm2_tom = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig5 = plt.figure(figsize=(12, 8))\n",
    "fig5 = sm.graphics.plot_partregress_grid(lm2_tom, fig=fig5)\n",
    "#fig5\n",
    "#fig5.savefig('Tomorrow_High_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts high value based on train data and model above\n",
    "high_prediction_tom = lm2_tom.predict(tomorrow_record)\n",
    "\n",
    "########################################################################################################################\n",
    "# Prepare formula to predict Low of stock data for tomorrow\n",
    "\n",
    "# Prepare formula to predict Low of stock data for today\n",
    "highest_sentiment3_tom, significant_value3_tom = identify_sig_feature_4_tomorrow(\"Low\", \"False\")\n",
    "\n",
    "# Update our 'Low' formula with the most significant feature based on tomorrow\n",
    "formula = ('Low ~ Open + Close + High + ' + np.unicode(highest_sentiment3_tom))\n",
    "\n",
    "# Update our training data for 'Low'\n",
    "dta = train_data_tomorrow[['Low', 'Open', 'Close', 'High', 'Anger', 'Anticipation',\n",
    "                           'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                           'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Update our training data for 'Low'\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment3_tom, \"Low\", significant_value3_tom)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm3_tom = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig6 = plt.figure(figsize=(12, 8))\n",
    "fig6 = sm.graphics.plot_partregress_grid(lm3_tom, fig=fig6)\n",
    "#fig6\n",
    "#fig6.savefig('Tomorrow_Low_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts Low value based on train data and model above\n",
    "low_prediction_tom = lm3_tom.predict(tomorrow_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our predicting, now let's print results. Unforuntately, we will not be able to possibly validate our predictions until the next buisness day closes of our stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Close value for tomorrow's stock is estimated to be: \" + str(close_prediction_tom.iloc[0]))\n",
    "print(\"The High value for tomorrow's stock is estimated to be: \" + str(high_prediction_tom.iloc[0]))\n",
    "print(\"The Low value for tomorrow's stock is estimated to be: \" + str(low_prediction_tom.iloc[0]))\n",
    "print(\"\")\n",
    "\n",
    "# Should We Buy or Sell? :)\n",
    "\n",
    "if float(today_close_prediction.iloc[0]) < float(close_prediction_tom.iloc[0]):\n",
    "    print(\"Based on our algorithm, the Closing value for the stock tomorrow will: Increase\")\n",
    "else:\n",
    "    print(\"Based on our algorithm, the Closing value for the stock tomorrow will: Decrease\")\n",
    "\n",
    "if float(today_high_prediction.iloc[0]) < float(high_prediction_tom.iloc[0]):\n",
    "    print(\"Based on our algorithm, the High value for the stock tomorrow will: Increase\")\n",
    "else:\n",
    "    print(\"Based on our algorithm, the High value for the stock tomorrow will: Decrease\")\n",
    "\n",
    "if float(today_low_prediction.iloc[0]) < float(low_prediction_tom.iloc[0]):\n",
    "    print(\"Based on our algorithm, the Low value for the stock tomorrow will: Increase\")\n",
    "else:\n",
    "    print(\"Based on our algorithm, the Low value for the stock tomorrow will: Decrease\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
