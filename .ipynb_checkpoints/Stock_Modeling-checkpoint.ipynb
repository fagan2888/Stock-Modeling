{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Final Project: Stock Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook models stock data that has been pre-processed with news sentiment data, then predicts the Closing, High, and Low values for the current buisness day and following business day. Below is the process for preparing the data then modeling the data for the best predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height has been deprecated.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj.Close</th>\n",
       "      <th>change</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>...</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Max_Sentiment</th>\n",
       "      <th>Cycle_Change</th>\n",
       "      <th>Sentiment_Proportion</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/11/2008</td>\n",
       "      <td>11729.66992</td>\n",
       "      <td>11867.11035</td>\n",
       "      <td>11675.53027</td>\n",
       "      <td>11782.34961</td>\n",
       "      <td>183190000</td>\n",
       "      <td>11782.34961</td>\n",
       "      <td>0.449115</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/12/2008</td>\n",
       "      <td>11781.70020</td>\n",
       "      <td>11782.34961</td>\n",
       "      <td>11601.51953</td>\n",
       "      <td>11642.46973</td>\n",
       "      <td>173590000</td>\n",
       "      <td>11642.46973</td>\n",
       "      <td>-1.181752</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Fear</td>\n",
       "      <td>False</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/13/2008</td>\n",
       "      <td>11632.80957</td>\n",
       "      <td>11633.78027</td>\n",
       "      <td>11453.33984</td>\n",
       "      <td>11532.95996</td>\n",
       "      <td>182550000</td>\n",
       "      <td>11532.95996</td>\n",
       "      <td>-0.858345</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/14/2008</td>\n",
       "      <td>11532.07031</td>\n",
       "      <td>11718.28027</td>\n",
       "      <td>11450.88965</td>\n",
       "      <td>11615.92969</td>\n",
       "      <td>159790000</td>\n",
       "      <td>11615.92969</td>\n",
       "      <td>0.727184</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8/15/2008</td>\n",
       "      <td>11611.20996</td>\n",
       "      <td>11709.88965</td>\n",
       "      <td>11599.73047</td>\n",
       "      <td>11659.90039</td>\n",
       "      <td>215040000</td>\n",
       "      <td>11659.90039</td>\n",
       "      <td>0.419340</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>False</td>\n",
       "      <td>0.180952</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date         Open         High          Low        Close     Volume    Adj.Close    change  Anger  Anticipation  ...   Sadness  Surprise  Trust  Negative  Positive  Max_Sentiment  Cycle_Change  Sentiment_Proportion False  True\n",
       "1  8/11/2008  11729.66992  11867.11035  11675.53027  11782.34961  183190000  11782.34961  0.449115   13.0           7.0  ...       9.0       4.0    8.0      20.0       9.0       Negative         False              0.212766     1     0\n",
       "2  8/12/2008  11781.70020  11782.34961  11601.51953  11642.46973  173590000  11642.46973 -1.181752    6.0           6.0  ...       4.0       5.0    7.0      11.0      11.0           Fear         False              0.161765     1     0\n",
       "3  8/13/2008  11632.80957  11633.78027  11453.33984  11532.95996  182550000  11532.95996 -0.858345   10.0           3.0  ...       8.0       3.0    5.0      14.0      11.0       Negative          True              0.212121     0     1\n",
       "4  8/14/2008  11532.07031  11718.28027  11450.88965  11615.92969  159790000  11615.92969  0.727184    8.0           4.0  ...       7.0       2.0   11.0      15.0      15.0       Negative         False              0.187500     1     0\n",
       "5  8/15/2008  11611.20996  11709.88965  11599.73047  11659.90039  215040000  11659.90039  0.419340   10.0          12.0  ...       9.0       6.0   12.0      16.0      19.0       Positive         False              0.180952     1     0\n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "# Set Console formatting for panda prints\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# **********************************************************************************************************************\n",
    "# Modeling / Prepare Data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/mwilchek/Stock-Modeling/master/DJ_NEWS_SENTIMENT_DATA.csv')\n",
    "data['Cycle_Change'] = data.Max_Sentiment.eq(data.Max_Sentiment.shift())\n",
    "dummies = pd.get_dummies(data.Cycle_Change)\n",
    "data = data.join(dummies)\n",
    "data_tomorrow = data\n",
    "\n",
    "# Move certain columns up by one row for data_tomorrow\n",
    "data_tomorrow.Anger = data_tomorrow.Anger.shift(+1)\n",
    "data_tomorrow.Anticipation = data_tomorrow.Anticipation.shift(+1)\n",
    "data_tomorrow.Disgust = data_tomorrow.Disgust.shift(+1)\n",
    "data_tomorrow.Fear = data_tomorrow.Fear.shift(+1)\n",
    "data_tomorrow.Joy = data_tomorrow.Joy.shift(+1)\n",
    "data_tomorrow.Sadness = data_tomorrow.Sadness.shift(+1)\n",
    "data_tomorrow.Surprise = data_tomorrow.Surprise.shift(+1)\n",
    "data_tomorrow.Trust = data_tomorrow.Trust.shift(+1)\n",
    "data_tomorrow.Negative = data_tomorrow.Negative.shift(+1)\n",
    "data_tomorrow.Positive = data_tomorrow.Positive.shift(+1)\n",
    "data_tomorrow.Max_Sentiment = data_tomorrow.Max_Sentiment.shift(+1)\n",
    "data_tomorrow.Sentiment_Proportion = data_tomorrow.Sentiment_Proportion.shift(+1)\n",
    "\n",
    "# Delete the first row of data_tomorrow\n",
    "data_tomorrow.drop(data_tomorrow.head(1).index, inplace=True)\n",
    "\n",
    "train_data = data[:-1]  # train data\n",
    "today_record = data.tail(1)  # test data (validate current day and predict from following day)\n",
    "train_data_tomorrow = data_tomorrow[:-1]  # train data\n",
    "tomorrow_record = data_tomorrow.tail(1)  # test data (validate current day and predict from following day)\n",
    "\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Local Functions for Accuracy Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Local method to get Margin of Error\n",
    "def get_change(current, previous):\n",
    "    if current == previous:\n",
    "        return 100.0\n",
    "    try:\n",
    "        return (abs(current - previous) / previous) * 100.0\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Modeling Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we created a pipeline of Regressor type models with a number of parameters that we tune to hopefully find an accurate model for predicting the closing value. We hope that with good results, we can mimic the process for predicting the High and Low values for the current and next business day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling: Pipeline(memory=None,\n",
      "     steps=[('StandardScaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('regr', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False))])\n",
      "Modeling Completed - Appending scores...\n",
      "Modeling: Pipeline(memory=None,\n",
      "     steps=[('StandardScaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('regr', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       lear...=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "E:\\Programming\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling Completed - Appending scores...\n",
      "Modeling: Pipeline(memory=None,\n",
      "     steps=[('StandardScaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('regr', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=0, splitter='best'))])\n",
      "Modeling Completed - Appending scores...\n",
      "Modeling: Pipeline(memory=None,\n",
      "     steps=[('StandardScaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('regr', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
      "           oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Modeling Completed - Appending scores...\n",
      "Modeling: Pipeline(memory=None,\n",
      "     steps=[('StandardScaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('regr', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Modeling Completed - Appending scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# MODELING EXPLORATION #################################################################################################\n",
    "# Testing best model for f(x) = Close ~ Features\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Get Feature values\n",
    "x = data[['Open', 'High', 'Low', False, True]].values\n",
    "\n",
    "# Get Target values\n",
    "y = data['Close'].values\n",
    "\n",
    "regression_models = {'lr': LinearRegression(n_jobs=-1),\n",
    "                     'mlp': MLPRegressor(random_state=0),\n",
    "                     'dt': DecisionTreeRegressor(random_state=0),\n",
    "                     'rf': RandomForestRegressor(random_state=0, n_jobs=-1),\n",
    "                     'svr': SVR(max_iter=-1)}\n",
    "\n",
    "pipe_regrs = {}\n",
    "\n",
    "# Create list of pipeline models to test with that standardize the data\n",
    "for name, regression_models in regression_models.items():\n",
    "    pipe_regrs[name] = Pipeline([('StandardScaler', StandardScaler()), ('regr', regression_models)])\n",
    "\n",
    "param_grids = {}\n",
    "\n",
    "# Linear Regression Parameter Options:\n",
    "param_grid = [{'regr__normalize': ['True']},\n",
    "              {'regr__normalize': ['False']}]\n",
    "\n",
    "# Add Linear Regression Parameters to dictionary grid\n",
    "param_grids['lr'] = param_grid\n",
    "\n",
    "# MLP Parameter Options:\n",
    "alpha_range = [10 ** i for i in range(-4, 5)]\n",
    "\n",
    "param_grid = [{'regr__hidden_layer_sizes': [10, 100, 200]}]\n",
    "\n",
    "# Add Multi-layer Perceptron Parameters to dictionary grid\n",
    "param_grids['mlp'] = param_grid\n",
    "\n",
    "# Decision Tree Regression Parameter Options:\n",
    "param_grid = [{'regr__criterion': ['mse', 'mae'],\n",
    "               'regr__min_samples_split': [2, 6, 10],\n",
    "               'regr__min_samples_leaf': [1, 6, 10],\n",
    "               'regr__max_features': ['auto', 'sqrt', 'log2']}]\n",
    "\n",
    "# Add Decision Tree Parameters to dictionary grid\n",
    "param_grids['dt'] = param_grid\n",
    "\n",
    "# Random Forest Regression Parameter Options:\n",
    "param_grid = [{'regr__n_estimators': [10, 100],\n",
    "               'regr__criterion': ['mse', 'mae'],\n",
    "               'regr__min_samples_split': [2, 6, 10],\n",
    "               'regr__min_samples_leaf': [1, 6, 10],\n",
    "               'regr__max_features': ['auto', 'sqrt', 'log2']}]\n",
    "\n",
    "# Add Random Forest Parameters to dictionary grid\n",
    "param_grids['rf'] = param_grid\n",
    "\n",
    "# Support Vector Machine (SVM) Parameter Options:\n",
    "param_grid = [{'regr__C': [0.1, 1, 10],\n",
    "               'regr__gamma': [0.1, 1, 10],\n",
    "               'regr__kernel': ['linear', 'poly', 'rbf', 'sigmoid']}]\n",
    "\n",
    "# Add SVM Parameters to dictionary grid\n",
    "param_grids['svr'] = param_grid\n",
    "\n",
    "# The list of [best_score_, best_params_, best_estimator_]\n",
    "best_score_param_estimators = []\n",
    "\n",
    "# Scoring Param: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "# For each regression\n",
    "for name in pipe_regrs.keys():\n",
    "    # GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipe_regrs[name],\n",
    "                      param_grid=param_grids[name],\n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      n_jobs=1,\n",
    "                      cv=None)\n",
    "    print(\"Modeling: \" + str(pipe_regrs[name]))\n",
    "    # Fit the pipeline\n",
    "    gs = gs.fit(x, y)\n",
    "\n",
    "    # Update best_score_param_estimators\n",
    "    best_score_param_estimators.append([gs.best_score_, gs.best_params_, gs.best_estimator_])\n",
    "    print(\"Modeling Completed - Appending scores...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the user's computer, the amount of time to complete the Pipeline can be between 5-10mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2954.0442867434845, {'regr__normalize': 'True'}, <class 'sklearn.linear_model.base.LinearRegression'>]\n",
      "\n",
      "[-8390.9515370176341, {'regr__C': 10, 'regr__gamma': 0.1, 'regr__kernel': 'linear'}, <class 'sklearn.svm.classes.SVR'>]\n",
      "\n",
      "[-2360217.4331258601, {'regr__criterion': 'mse', 'regr__max_features': 'sqrt', 'regr__min_samples_leaf': 1, 'regr__min_samples_split': 6}, <class 'sklearn.tree.tree.DecisionTreeRegressor'>]\n",
      "\n",
      "[-2401253.8019238515, {'regr__criterion': 'mse', 'regr__max_features': 'auto', 'regr__min_samples_leaf': 1, 'regr__min_samples_split': 2, 'regr__n_estimators': 100}, <class 'sklearn.ensemble.forest.RandomForestRegressor'>]\n",
      "\n",
      "[-138227235.68765172, {'regr__hidden_layer_sizes': 200}, <class 'sklearn.neural_network.multilayer_perceptron.MLPRegressor'>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort best_score_param_estimators in descending order of the best_score_\n",
    "best_score_param_estimators = sorted(best_score_param_estimators, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# For each [best_score_, best_params_, best_estimator_]\n",
    "for best_score_param_estimator in best_score_param_estimators:\n",
    "    # Print out [best_score_, best_params_, best_estimator_], where best_estimator_ is a pipeline\n",
    "    # Since we only print out the type of classifier of the pipeline\n",
    "    print([best_score_param_estimator[0], best_score_param_estimator[1],\n",
    "           type(best_score_param_estimator[2].named_steps['regr'])], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the '{'regr__normalize': 'True'}, <class 'sklearn.linear_model.base.LinearRegression'>]' model from the Pipeline was the best scored. Let us practice predicting the cclosing value of the stock with a Linear Regression model with the best tuned parameters from our GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Closing Value: 25413.2207\n",
      "Predicted Closing Value: 25398.6424397\n",
      "Accuracy error for prediction: 0.0574%\n"
     ]
    }
   ],
   "source": [
    "# Declare best model from GridSearchCV where normalize set to True is the default parameter\n",
    "lr = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# Fit the model with our data\n",
    "lr = lr.fit(x, y)\n",
    "\n",
    "# Predict on Today Close\n",
    "today_close = today_record[['Open', 'High', 'Low', False, True]].values\n",
    "y_pred = lr.predict(today_close)\n",
    "\n",
    "# Print Results\n",
    "print(\"Actual Closing Value: \" + str(today_record['Close'].values[0]))\n",
    "print(\"Predicted Closing Value: \" + str(y_pred[0]))\n",
    "\n",
    "error = get_change(y_pred[0], today_record['Close'].values[0])\n",
    "print(\"Accuracy error for prediction: \" + str(round(error, 4)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction results were pretty close to the actual. However, based on academic research we also want to explore OLS regression to see if there is any noticeable change in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Close   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 1.937e+06\n",
      "Date:                Sun, 18 Nov 2018   Prob (F-statistic):               0.00\n",
      "Time:                        22:47:27   Log-Likelihood:                -10896.\n",
      "No. Observations:                2013   AIC:                         2.180e+04\n",
      "Df Residuals:                    2008   BIC:                         2.183e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept                0.0474      5.629      0.008      0.993     -10.992      11.087\n",
      "Cycle_Change[T.True]     0.6753      2.449      0.276      0.783      -4.127       5.477\n",
      "Open                    -0.6301      0.015    -41.554      0.000      -0.660      -0.600\n",
      "High                     0.8539      0.014     60.097      0.000       0.826       0.882\n",
      "Low                      0.7762      0.012     65.083      0.000       0.753       0.800\n",
      "==============================================================================\n",
      "Omnibus:                      291.957   Durbin-Watson:                   2.207\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4139.955\n",
      "Skew:                           0.040   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.025   Cond. No.                     1.14e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.14e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# **********************************************************************************************#\n",
    "# OLS Regression Test\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define formula string for Stats-model API\n",
    "formula = 'Close ~ Open + High + Low + Cycle_Change'\n",
    "\n",
    "# Define Training Data\n",
    "dta = train_data[['Close', 'Open', 'High', 'Low', 'Anger', 'Anticipation',\n",
    "                  'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                  'Trust', 'Negative', 'Positive', 'Cycle_Change', False, True, 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Set the Model\n",
    "ols_today_close_model = smf.ols(formula=formula, data=dta).fit()\n",
    "\n",
    "# Print results\n",
    "print(ols_today_close_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model is clearly overfit based on the Adj. R Squared equaling to 1. In order to prevent overfitting, we will revise our model with a regularized fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update Model with Regularized Fit to prevent over-fitting; alpha and weight values were set.\n",
    "olsUpdate_today_close = smf.ols(formula=formula, data=dta).fit_regularized(alpha=10, L1_wt=.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we practice predicting on today's record with the revised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date         Open         High          Low       Close     Volume   Adj.Close    change  Anger  Anticipation  ...   Sadness  Surprise  Trust  Negative  Positive  Max_Sentiment  Cycle_Change  Sentiment_Proportion False  True\n",
      "2014  11/16/2018  25242.34961  25510.23047  25147.80078  25413.2207  354461249  25413.2207  0.677442  380.0         345.0  ...     287.0     248.0  417.0     649.0     562.0       Negative          True              0.174135     0     1\n",
      "\n",
      "[1 rows x 23 columns]\n",
      "Predicted Close Value: 2014    25249.479971\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "print(today_record)\n",
    "\n",
    "olsUpdate_today_close_prediction = olsUpdate_today_close.predict(today_record)\n",
    "\n",
    "print(\"Predicted Close Value: \" + str(olsUpdate_today_close_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear an OLS Regularized fit regression model is quite accurate as well to the actual value based on the above plots. If we can hyperpameter tune the sentiment values in our formula, and the alpha and weight values for the regularized parameters we may be able to create a stronger model for predicting stock values with news sentiment data as a significant relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Hyperparameter Tuning for OLS Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will create custom local methods that will find the most significant sentiment for today's stock values in Close, High, and Low using a RandForestRegressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "########################################################################################################################\n",
    "# TODAY: Local method to identify most significant feature in dataset compared to y\n",
    "def identify_sig_feature_4_today(y_variable, graph_data):\n",
    "    \n",
    "    # Split Data Into X, which are ALL the features\n",
    "    x = data.iloc[:, 9:18].values\n",
    "\n",
    "    # Split Data Into y, which are the associated targets/classifications; looking at Volume\n",
    "    y = data[np.unicode(y_variable)].values\n",
    "\n",
    "    # Get the Column Names for Sentiment, Ignore Index\n",
    "    feat_labels = data.columns[9:19]\n",
    "\n",
    "    # Randomly choose 20% of the data for testing; want a large train set (set random_state as 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Declare the StandardScaler\n",
    "    std_scaler = StandardScaler()\n",
    "\n",
    "    # Standardize the features in the training data\n",
    "    X_train = std_scaler.fit_transform(X_train)\n",
    "\n",
    "    # Standardize the features in testing data\n",
    "    X_test = std_scaler.transform(X_test)\n",
    "\n",
    "    # Start The Random Forest Regressor\n",
    "    treereg = RandomForestRegressor(n_estimators=100, max_depth=11, random_state=0)\n",
    "\n",
    "    # Execute The Data With The Random Forest Regressor\n",
    "    treereg.fit(X_train, y_train)\n",
    "\n",
    "    print('The accuracy of the random forest for today sentiment is: ' + str(treereg.score(X_test, y_test)))\n",
    "\n",
    "    # Get The Important Features From The Regressor\n",
    "    importances = treereg.feature_importances_\n",
    "\n",
    "    # Sort The Features By The Most Important\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Return data\n",
    "    df_cols = ['Sentiment', 'Importance']\n",
    "    master_df = pd.DataFrame(columns=df_cols)\n",
    "\n",
    "    for f in range(x.shape[1]):\n",
    "        sentiment = feat_labels[f]\n",
    "        importance = importances[indices[f]]\n",
    "        temp_data = {'Sentiment': sentiment,\n",
    "                     'Importance': importance}\n",
    "        master_df = master_df.append(temp_data, ignore_index=True)\n",
    "\n",
    "    highest_sentiment = master_df['Sentiment'].iloc[0]\n",
    "    highest_importance = master_df['Importance'].iloc[0]\n",
    "\n",
    "    if graph_data == \"TRUE\":\n",
    "        # Output Data As A Plot for Overall Data set\n",
    "        plt.title('Today Feature Importances ' + np.unicode(y_variable))\n",
    "        plt.bar(range(x.shape[1]), importances[indices], color='lightblue', align='center')\n",
    "        plt.xticks(range(x.shape[1]), feat_labels, rotation=90)\n",
    "        plt.xlim([-1, x.shape[1]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return highest_sentiment, highest_importance\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# TOMORROW: Local method to identify most significant feature in dataset compared to y\n",
    "def identify_sig_feature_4_tomorrow(y_variable, graph_data):\n",
    "    \n",
    "    # Split Data Into X, which are ALL the features\n",
    "    x = data_tomorrow.iloc[:, 9:18].values\n",
    "\n",
    "    # Split Data Into y, which are the associated targets/classifications; looking at Volume\n",
    "    y = data_tomorrow[np.unicode(y_variable)].values\n",
    "\n",
    "    # Get the Column Names for Sentiment, Ignore Index\n",
    "    feat_labels = data_tomorrow.columns[9:19]\n",
    "\n",
    "    # Randomly choose 20% of the data for testing; want a large train set (set random_state as 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # Declare the StandardScaler\n",
    "    std_scaler = StandardScaler()\n",
    "\n",
    "    # Standardize the features in the training data\n",
    "    X_train = std_scaler.fit_transform(X_train)\n",
    "\n",
    "    # Standardize the features in testing data\n",
    "    X_test = std_scaler.transform(X_test)\n",
    "\n",
    "    # Start The Random Forest Regressor\n",
    "    treereg = RandomForestRegressor(n_estimators=100, max_depth=11, random_state=1)\n",
    "\n",
    "    # Execute The Data With The Random Forest Regressor\n",
    "    treereg.fit(X_train, y_train)\n",
    "\n",
    "    print('The accuracy of the random forest for tomorrow sentiment is: ' + str(treereg.score(X_test, y_test)))\n",
    "\n",
    "    # Get The Important Features From The Regressor\n",
    "    importances = treereg.feature_importances_\n",
    "\n",
    "    # Sort The Features By The Most Important\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Return data\n",
    "    df_cols = ['Sentiment', 'Importance']\n",
    "    master_df = pd.DataFrame(columns=df_cols)\n",
    "\n",
    "    for f in range(x.shape[1]):\n",
    "        sentiment = feat_labels[f]\n",
    "        importance = importances[indices[f]]\n",
    "        temp_data = {'Sentiment': sentiment,\n",
    "                     'Importance': importance}\n",
    "        master_df = master_df.append(temp_data, ignore_index=True)\n",
    "\n",
    "    highest_sentiment = master_df['Sentiment'].iloc[0]\n",
    "    highest_importance = master_df['Importance'].iloc[0]\n",
    "\n",
    "    if graph_data == \"TRUE\":\n",
    "        # Output Data As A Plot for Overall Data set\n",
    "        plt.title('Tomorrow Feature Importances ' + np.unicode(y_variable))\n",
    "        plt.bar(range(x.shape[1]), importances[indices], color='lightblue', align='center')\n",
    "        plt.xticks(range(x.shape[1]), feat_labels, rotation=90)\n",
    "        plt.xlim([-1, x.shape[1]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return highest_sentiment, highest_importance\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# Local method to correctly retrieve appropriate paramters for Regularized Fit Regression based on Ridge Regression\n",
    "def get_fit_regression_params(significant_sentiment, target_variable, sentiment_value):\n",
    "    \n",
    "    # Define the data needed for this section, and as defined by highest_sentiment\n",
    "    x = data[significant_sentiment].values.reshape(-1, 1)\n",
    "\n",
    "    y = data[np.unicode(target_variable)].values  # used to be just data.High\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    x_std = scaler.fit_transform(x)\n",
    "    \n",
    "    # Create ridge regression with alpha values from .1 to 10.0, in increments of 0.1\n",
    "    regr_cv = RidgeCV(alphas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0,\n",
    "                              1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0,\n",
    "                              2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0,\n",
    "                              3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.0,\n",
    "                              4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5.0,\n",
    "                              5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6.0,\n",
    "                              6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 7.0,\n",
    "                              7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9, 8.0,\n",
    "                              8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9.0,\n",
    "                              9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9, 10.0])\n",
    "\n",
    "    # Place x and y variables in the proper format for model_cv.\n",
    "    y = np.array(y)\n",
    "    x_std = x_std.reshape((len(y), 1))\n",
    "    y = y.reshape((len(y), 1))\n",
    "\n",
    "    # Determine the best alpha value to use.\n",
    "    model_cv = regr_cv.fit(x_std, y)\n",
    "    alpha_val_today = model_cv.alpha_\n",
    "\n",
    "    # Set the L1 value based on significant_sentiment_value\n",
    "    if sentiment_value >= 0.7:\n",
    "        weight_value = 0.4\n",
    "    elif sentiment_value >= 0.4:\n",
    "        weight_value = 0.5\n",
    "    else:\n",
    "        weight_value = 0.6\n",
    "\n",
    "    return alpha_val_today, weight_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the local functions to tune the parameters for our prediction formula and regularized fit parameters, let's practice again on the Today's closing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: OLS Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the random forest for today sentiment is: 0.196933566804\n",
      "The accuracy of the random forest for today sentiment is: 0.195502657764\n",
      "The accuracy of the random forest for today sentiment is: 0.192309650914\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# MODELING 4 TODAY #####################################################################################################\n",
    "# Prepare formula to predict closing of stock data for today\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'Close' for today\n",
    "highest_sentiment1_today, significant_value1_today = identify_sig_feature_4_today(\"Close\", \"False\")\n",
    "\n",
    "# Update our 'Close' formula with the most significant feature based on today's data\n",
    "formula = ('Close ~ Open + High + Low + ' + np.unicode(highest_sentiment1_today))\n",
    "\n",
    "# Update our training data for 'Close'\n",
    "dta = train_data[['Close', 'Open', 'High', 'Low', 'Anger', 'Anticipation',\n",
    "                  'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                  'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'Close' and today.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment1_today, \"Close\", significant_value1_today)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm1_today = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig1 = plt.figure(figsize=(12, 8))\n",
    "fig1 = sm.graphics.plot_partregress_grid(lm1_today, fig=fig1)\n",
    "#fig1\n",
    "#fig1.savefig('Today_Close_Regression.png')\n",
    "\n",
    "# Predicts closing value based on train data and model above\n",
    "today_close_prediction = lm1_today.predict(today_record)\n",
    "\n",
    "########################################################################################################################\n",
    "# Prepare formula to predict High of stock data for today\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'High' for today \n",
    "highest_sentiment2_today, significant_value2_today = identify_sig_feature_4_today(\"High\", \"False\")\n",
    "\n",
    "# Update our 'High' formula with the most significant feature based on today's data\n",
    "formula = ('High ~ Open + Close + Low + ' + np.unicode(highest_sentiment2_today))\n",
    "\n",
    "# Update our training data for 'High'\n",
    "dta = train_data[['High', 'Open', 'Close', 'Low', 'Anger', 'Anticipation',\n",
    "                  'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                  'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'High' and today.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment2_today, \"High\", significant_value2_today)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm2_today = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig2 = plt.figure(figsize=(12, 8))\n",
    "fig2 = sm.graphics.plot_partregress_grid(lm2_today, fig=fig2)\n",
    "#fig2\n",
    "#fig2.savefig('Today_High_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts high value based on train data and model above\n",
    "today_high_prediction = lm2_today.predict(today_record)\n",
    "\n",
    "########################################################################################################################\n",
    "# Prepare formula to predict Low of stock data for today\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'Low' for today \n",
    "highest_sentiment3_today, significant_value3_today = identify_sig_feature_4_today(\"Low\", \"False\")\n",
    "\n",
    "# Update our 'Low' formula with the most significant feature based on today's data\n",
    "formula = ('Low ~ Open + Close + High + ' + np.unicode(highest_sentiment3_today))\n",
    "\n",
    "# Update our training data for 'Low'\n",
    "dta = train_data[['Low', 'Open', 'Close', 'High', 'Anger', 'Anticipation',\n",
    "                  'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                  'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'Low' and today.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment3_today, \"Low\", significant_value3_today)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm3_today = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig3 = plt.figure(figsize=(12, 8))\n",
    "fig3 = sm.graphics.plot_partregress_grid(lm3_today, fig=fig3)\n",
    "#fig3\n",
    "#fig3.savefig('Today_Low_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts Low value based on train data and model above\n",
    "today_low_prediction = lm3_today.predict(today_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our predicting, now let's print results and compare with actuals..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Close value for today's stock is predicted to be: 25256.0766054\n",
      "The High value for today's stock is predicted to be: 25521.897871\n",
      "The Low value for today's stock is predicted to be: 24950.1117627\n",
      "\n",
      "ACTUAL Close value for today: 25413.2207\n",
      "ACTUAL High value for today: 25510.23047\n",
      "ACTUAL Low value for today: 25147.80078\n"
     ]
    }
   ],
   "source": [
    "print(\"The Close value for today's stock is predicted to be: \" + str(today_close_prediction.iloc[0]))\n",
    "print(\"The High value for today's stock is predicted to be: \" + str(today_high_prediction.iloc[0]))\n",
    "print(\"The Low value for today's stock is predicted to be: \" + str(today_low_prediction.iloc[0]))\n",
    "print(\"\")\n",
    "print(\"ACTUAL Close value for today: \" + str(today_record['Close'].iloc[0]))\n",
    "print(\"ACTUAL High value for today: \" + str(today_record['High'].iloc[0]))\n",
    "print(\"ACTUAL Low value for today: \" + str(today_record['Low'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our results are pretty good. Let's re-use this methodology to actually try to predict for tomorrow's Closing, High, and Low values of the stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the random forest for tomorrow sentiment is: 0.126216767683\n",
      "The accuracy of the random forest for tomorrow sentiment is: 0.128234792931\n",
      "The accuracy of the random forest for tomorrow sentiment is: 0.136239495924\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# MODELING 4 NEXT DAY###################################################################################################\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'Close' for tomorrow \n",
    "highest_sentiment1_tom, significant_value1_tom = identify_sig_feature_4_tomorrow(\"Close\", \"False\")\n",
    "\n",
    "# Update our 'Close' formula with the most significant feature based on tomorrow\n",
    "formula = ('Close ~ Open + High + Low + ' + np.unicode(highest_sentiment1_tom))\n",
    "\n",
    "# Update our training data for 'Close'\n",
    "dta = train_data_tomorrow[['Close', 'Open', 'High', 'Low', 'Anger', 'Anticipation',\n",
    "                           'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                           'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'Close' and tomorrow.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment1_tom, \"Close\", significant_value1_tom)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm1_tom = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig4 = plt.figure(figsize=(12, 8))\n",
    "fig4 = sm.graphics.plot_partregress_grid(lm1_tom, fig=fig4)\n",
    "#fig4\n",
    "#fig4.savefig('Tomorrow_Close_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts closing value based on train data and model above\n",
    "close_prediction_tom = lm1_tom.predict(tomorrow_record)\n",
    "\n",
    "########################################################################################################################\n",
    "# Prepare formula to predict High of stock data for tomorrow\n",
    "\n",
    "# Get the highest sentiment and most significant feature against 'High' for tomorrow \n",
    "highest_sentiment2_tom, significant_value2_tom = identify_sig_feature_4_tomorrow(\"High\", \"False\")\n",
    "\n",
    "# Update our 'High' formula with the most significant feature based on tomorrow\n",
    "formula = ('High ~ Open + Close + Low + ' + np.unicode(highest_sentiment2_tom))\n",
    "\n",
    "# Update our training data for 'High'\n",
    "dta = train_data_tomorrow[['High', 'Open', 'Close', 'Low', 'Anger', 'Anticipation',\n",
    "                           'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                           'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Get best regularized fit paramters based on most significant sentiment feature for 'High' and tomorrow.\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment2_tom, \"High\", significant_value2_tom)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm2_tom = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig5 = plt.figure(figsize=(12, 8))\n",
    "fig5 = sm.graphics.plot_partregress_grid(lm2_tom, fig=fig5)\n",
    "#fig5\n",
    "#fig5.savefig('Tomorrow_High_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts high value based on train data and model above\n",
    "high_prediction_tom = lm2_tom.predict(tomorrow_record)\n",
    "\n",
    "########################################################################################################################\n",
    "# Prepare formula to predict Low of stock data for tomorrow\n",
    "\n",
    "# Prepare formula to predict Low of stock data for today\n",
    "highest_sentiment3_tom, significant_value3_tom = identify_sig_feature_4_tomorrow(\"Low\", \"False\")\n",
    "\n",
    "# Update our 'Low' formula with the most significant feature based on tomorrow\n",
    "formula = ('Low ~ Open + Close + High + ' + np.unicode(highest_sentiment3_tom))\n",
    "\n",
    "# Update our training data for 'Low'\n",
    "dta = train_data_tomorrow[['Low', 'Open', 'Close', 'High', 'Anger', 'Anticipation',\n",
    "                           'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise',\n",
    "                           'Trust', 'Negative', 'Positive', 'Sentiment_Proportion']].copy()\n",
    "\n",
    "# Update our training data for 'Low'\n",
    "alpha_val, weight_val = get_fit_regression_params(highest_sentiment3_tom, \"Low\", significant_value3_tom)\n",
    "\n",
    "# Create a Ordinary Least Squares regression model\n",
    "lm3_tom = smf.ols(formula=formula, data=dta).fit_regularized(alpha=alpha_val, L1_wt=weight_val)\n",
    "\n",
    "# Print regression graph\n",
    "fig6 = plt.figure(figsize=(12, 8))\n",
    "fig6 = sm.graphics.plot_partregress_grid(lm3_tom, fig=fig6)\n",
    "#fig6\n",
    "#fig6.savefig('Tomorrow_Low_Regression.png')  # Show Partial regression plot of model\n",
    "\n",
    "# Predicts Low value based on train data and model above\n",
    "low_prediction_tom = lm3_tom.predict(tomorrow_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our predicting, now let's print results. Unforuntately, we will not be able to possibly validate our predictions until the next buisness day closes of our stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Close value for tomorrow's stock is estimated to be: 25256.0766054\n",
      "The High value for tomorrow's stock is estimated to be: 25521.897871\n",
      "The Low value for tomorrow's stock is estimated to be: 24950.1117627\n",
      "\n",
      "Based on our algorithm, the Closing value for the stock tomorrow will: Decrease\n",
      "Based on our algorithm, the High value for the stock tomorrow will: Decrease\n",
      "Based on our algorithm, the Low value for the stock tomorrow will: Decrease\n"
     ]
    }
   ],
   "source": [
    "print(\"The Close value for tomorrow's stock is estimated to be: \" + str(close_prediction_tom.iloc[0]))\n",
    "print(\"The High value for tomorrow's stock is estimated to be: \" + str(high_prediction_tom.iloc[0]))\n",
    "print(\"The Low value for tomorrow's stock is estimated to be: \" + str(low_prediction_tom.iloc[0]))\n",
    "print(\"\")\n",
    "\n",
    "# Should We Buy or Sell? :)\n",
    "\n",
    "if float(today_close_prediction.iloc[0]) < float(close_prediction_tom.iloc[0]):\n",
    "    print(\"Based on our algorithm, the Closing value for the stock tomorrow will: Increase\")\n",
    "else:\n",
    "    print(\"Based on our algorithm, the Closing value for the stock tomorrow will: Decrease\")\n",
    "\n",
    "if float(today_high_prediction.iloc[0]) < float(high_prediction_tom.iloc[0]):\n",
    "    print(\"Based on our algorithm, the High value for the stock tomorrow will: Increase\")\n",
    "else:\n",
    "    print(\"Based on our algorithm, the High value for the stock tomorrow will: Decrease\")\n",
    "\n",
    "if float(today_low_prediction.iloc[0]) < float(low_prediction_tom.iloc[0]):\n",
    "    print(\"Based on our algorithm, the Low value for the stock tomorrow will: Increase\")\n",
    "else:\n",
    "    print(\"Based on our algorithm, the Low value for the stock tomorrow will: Decrease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
